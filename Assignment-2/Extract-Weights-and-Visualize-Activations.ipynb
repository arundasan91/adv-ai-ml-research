{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Weights and Visualize the Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import itertools\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as Functional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "N_train = 64\n",
    "N_test = 256\n",
    "\n",
    "# We will use torch.utils.data.DataLoader to wrap our dataset.\n",
    "# This provides easier batching, GPU support, etc.\n",
    "# Calling torchvision.datasets.MNIST() will download and format the MNIST\n",
    "# dataset with the transforms we specify. Here, in the transforms we first convert\n",
    "# the image to PyTorch tensor, and then normalize the image based on a given mean\n",
    "# and standard deviation. Normalizing the image does: image = (image - mean) / std.\n",
    "# We shuffle the data as well by defining shuffle=True.\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../Datasets/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=N_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../Datasets/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=N_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset = enumerate(test_loader)\n",
    "batch_idx, (one_batch_of_test_subset_x, one_batch_of_test_subset_y) = next(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "plt.imshow(one_batch_of_test_subset_x[i][0], cmap='gray', interpolation='none')\n",
    "_ = plt.title(\"Ground Truth: {}\".format(one_batch_of_test_subset_y[i]))\n",
    "number7 = one_batch_of_test_subset_x[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_A, self).__init__()\n",
    "        # We can define the arguments of each layer in the __init__ method.\n",
    "        # __init__ method will be called everytime we create an object of this class.\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(1600, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This is the forward pass function.\n",
    "        # See how we can save the activation outputs of each layer into a variable.\n",
    "        # In this case, we are saving the output of each layer\n",
    "        # to the same variable and replacing the value every time\n",
    "        # before sending to a new layers.\n",
    "        \n",
    "        # Conv -> MaxPool -> ReLU\n",
    "        x = self.conv1(x)\n",
    "        x = Functional.max_pool2d(x, 2)\n",
    "        x = Functional.relu(x)\n",
    "        \n",
    "        # Conv -> MaxPool -> ReLU -> Dropout -> Flatten\n",
    "        x = self.conv2(x)\n",
    "        x = Functional.max_pool2d(x, 2)\n",
    "        x = Functional.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # 3-layer MLP\n",
    "        x = self.fc1(x)\n",
    "        x = Functional.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = Functional.relu(x)      \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('cnn_a_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Weight Matrices/Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model):\n",
    "    \"\"\"\n",
    "    Returns the number of parameters (trainable and total) of a PyTorch model.\n",
    "    \"\"\"\n",
    "    print(\"Trainable parameter variables: {}\\nTotal number of parameters: {}\\nTotal number of trainable parameters: {}\".format(\n",
    "        len(list(model.parameters())),\n",
    "        sum(p.numel() for p in model.parameters()),\n",
    "        sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weights of the kernels of conv1 layer.\n",
    "# We can later see how the weights vary per each layer.\n",
    "kernels_conv1 = model.conv1.weight.cpu().detach().clone().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels_conv1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, i in enumerate(kernels_conv1):\n",
    "    plt.imshow(i[0], cmap='gray')\n",
    "    plt.show()\n",
    "    if _==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how we studied the embeddings in DNNs, in CNNs we are also interested in what kernels/filters the CNN had learnt during the optimization. To do so, we can do a forward pass by providing an input to the model and see what each kernels provide as an output. By visualizing these outputs, or activations, we can study (visually) what the network is learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Conv1 layer activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{W-K+2P}{S} +1$$\n",
    "\n",
    "- W is the input volume\n",
    "- K is the kernel size\n",
    "- P is the amount of padding\n",
    "- S is the stride size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a forward pass on the first convolution layer by passing\n",
    "# the original MNIST image data to it.\n",
    "\n",
    "conv1_activations = model.conv1.forward(number7.reshape(1,1,28,28)) # NCHW\n",
    "\n",
    "# Alternate way to extract activations.\n",
    "# conv1_layer = nn.Sequential(*list(model.children()))[0]\n",
    "# conv1_activations = conv1_layer(number7.reshape(1,1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv2 activations can be extracted by doing a forward pass on conv2 layer with conv1 activations as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_act_conv1 = conv1_activations.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_act_conv1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, i in enumerate(np_act_conv1):\n",
    "    plt.imshow(i, cmap='gray')\n",
    "    plt.show()\n",
    "    if _==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of Convolution Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_activations = model.conv1.forward(number7.reshape(1,1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling = nn.MaxPool2d(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_act_pool = pooling(conv1_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_act_pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of Convolution Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2_activations = model.conv2.forward(c1_act_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_act_pool = pooling(conv2_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_act_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, i in enumerate(conv2_activations.detach().numpy()[0]):\n",
    "    plt.imshow(i, cmap='gray')\n",
    "    plt.show()\n",
    "    if _==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of 1st Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_out = model.fc1.forward(c2_act_pool.reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_out_relu = relu(fc1_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of 2nd Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc2_out = model.fc2.forward(fc1_out_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is your embeddings!\n",
    "fc2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
